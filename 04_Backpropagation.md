# Chapter 04. Backpropagation

*if the input: f and output: q in backpropagation, the gradient of q means the effect of q on f*


## ğŸ¶ Review

- Loss Function
  - show how bad the model is
  - ex) SVM loss

- Optimization
  - to find W that minimalize loss(result of loss function)
 
```python
# Vanilla Gradient Descent
 
while True:
  weights_grad = evaluate_gradient(losS_fun, data, weights)
  weights += - step_size * weights_grad
```

- Gradient Descent

    - find gradient about W
    - Numerical Gradient: slow
    - Analytic Gradient: fast
      - ë¯¸ë¶„ìœ¼ë¡œ ê³µì‹ì„ ìœ ë„í•˜ì—¬ gradient ê³„ì‚°

Gradient = ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ëª¨ë“  ì…ë ¥ê°’ì—ì„œ ëª¨ë“  ë°©í–¥ì— ëŒ€í•œ ìˆœê°„ ë³€í™”ìœ¨ = í¸ë¯¸ë¶„ê°’ì˜ ë²¡í„°




## ğŸ¶ Computational Graph

: useful to catch the flow of front-back propagation

sigmoid gate: grouping

*gradient descent of end node: 1*


### Gate

**add** gate: gradient distributor / local gradient * upstream gradient

**mul** gate: swap multiplier / local gradient * upstream gradient of another input node

**copy** gate: gradient adder / output1 local gradient + output2 local gradient

**max** gate: gradient router / local gradient ë°›ì€ ìª½ìœ¼ë¡œ upstream gradient ì „ë‹¬, ë°˜ëŒ€ìª½ì€ 0



## ğŸ¶ Backpropagation

: recursive application of the chain rule along a computational graph to compute the gradients of all inputs/parameters/intermediates

- forward: compute & save

- backward: apply the chain rule to compute the gradient of the loss function with respect to the inputs

ë³€ìˆ˜ì˜ gradientëŠ” ë³€ìˆ˜ì™€ ê°™ì€ shape ê°€ì ¸ì•¼ í•¨ -> *check gradient shape == variable shape*


### Jacobian

derivative

diagonal matrix: ì¶œë ¥ì˜ í•´ë‹¹ ìš”ì†Œì—ë§Œ ì˜í–¥ì„ ì¤Œ


