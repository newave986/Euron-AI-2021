# Chapter 08. Deep Learning Software

## ğŸŒ± Review

- regularization - Dropout: forward passì—ì„œ ì„ì˜ì˜ ë¶€ë¶„ 0, test timeì—ì„œ  marginalize out
- regularization - basic idea: train time + noise, test time + marginalize out
- transfer learning

## ğŸŒ± CPU vs GPU

- **CPU**: central processing unit
	- small, samll size

- **GPU**: graphics card, graphics processing unit
	- rendering for computer graphics
	- big, big size
	- many power
	- deep learning: *NVIDIA*
		- cuBLAS, *cuDNN* library: don't have to code CUDA 
		- OpenCL(+ AMD, CPU)( < CUDA)

- CPU vs GPU

| | CPU | GPU |
|---|---|---|
| core | 4/6/10 | thousand (clock speed) |
| thread | 8~20 (+hyperthreading) | |

- CPUì˜ memoryëŠ” ë…ë¦½ì , GPUëŠ” ë³‘ë ¬ì 
	- ë³‘ë ¬í™” ë¬¸ì œì—ì„œ GPU ì²˜ë¦¬ëŸ‰ ì••ë„ì 
	- GPUëŠ” ê°™ì€ ë¬¸ì œì— ëŒ€í•˜ì—¬ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨
	- Convolution ì—°ì‚°ì„ ê° coreì— ë¶„ë°°ì‹œì¼œ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ í•¨

- GPU problem
	- Model, Weight: GPU RAM / Train Data: SSD
		- **bottleneck**
	- solution
		- data set -> RAM (if data set is small)
		- or HHD -> SSD
		- **pre-fetching**
		- buffer -> GPU data



## ğŸŒ± Deep Learning Frameworks

I. not have to make complex Graph On Our Own

II. computating Gradient - automatically (loss and weight gradient of loss) - only have to compute forward pass, back propagation computed automatically

III. GPU efficiently - CPU -> GPU: easy

- 1. academia
	- Berkely: Caffe
	- NYU + facebook: Torch

- 2. industry
	- Facebook: Caffe2, PyTorch
	- Google: TensorFlow
	- Baidu: Paddle
	- Microsoft: CNTK
	- Amazon: MXNet

- academia to industry


